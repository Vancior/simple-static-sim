{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%load_ext blackcellmagic\r\n",
    "import sys\r\n",
    "import uuid\r\n",
    "sys.path.insert(0, \"..\")\r\n",
    "def gen_uuid():\r\n",
    "    return str(uuid.uuid4())[:8]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import algo\r\n",
    "import coloredlogs\r\n",
    "import graph\r\n",
    "import importlib\r\n",
    "import logging\r\n",
    "import networkx as nx\r\n",
    "import schedule as sch\r\n",
    "import random\r\n",
    "import topo\r\n",
    "import yaml\r\n",
    "importlib.reload(algo)\r\n",
    "importlib.reload(graph)\r\n",
    "importlib.reload(sch)\r\n",
    "importlib.reload(topo)\r\n",
    "coloredlogs.set_level(logging.INFO)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "gen_args = {\r\n",
    "    \"graph_length\": 5,\r\n",
    "    \"mi_cb\": lambda: 10,\r\n",
    "    \"memory_cb\": lambda: int(1e8),\r\n",
    "    \"unit_size_cb\": lambda: random.randint(1000, 10000),\r\n",
    "    \"unit_rate_cb\": lambda: random.randint(1, 100),\r\n",
    "    \"source_hosts\": [\"rasp1\"],\r\n",
    "    \"sink_hosts\": [\"cloud1\"],\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "sc = topo.Scenario.from_dict(yaml.load(open(\"../samples/a0.yaml\", \"r\").read(), Loader=yaml.Loader))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "gen_args_list = [\r\n",
    "    {\r\n",
    "        \"graph_length\": random.randint(3, 11),\r\n",
    "        \"mi_cb\": lambda: 1,\r\n",
    "        \"memory_cb\": lambda: int(1e8),\r\n",
    "        \"unit_size_cb\": lambda: random.randint(10000, 100000),\r\n",
    "        \"unit_rate_cb\": lambda: random.randint(1, 10),\r\n",
    "        \"source_hosts\": [\"rasp1\"],\r\n",
    "        \"sink_hosts\": [\"cloud1\"],\r\n",
    "    }\r\n",
    "    for _ in range(10)\r\n",
    "]\r\n",
    "graph_list = [graph.GraphGenerator(gen_uuid(), **gen_args).gen_random_chain_graph() for gen_args in gen_args_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "min_cut_scheduler = sch.RandomScheduler()\r\n",
    "min_cut_scheduler.logger.setLevel(logging.INFO)\r\n",
    "min_cut_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "min_cut_calculator.logger.setLevel(logging.INFO)\r\n",
    "min_cut_result_list = []\r\n",
    "for g in graph_list:\r\n",
    "    s_cut, t_cut = algo.min_cut(g)\r\n",
    "    print(\"graph {}: s {} t {}\".format(g.uuid, len(s_cut), len(t_cut)))\r\n",
    "    s_graph = g.sub_graph(s_cut, gen_uuid())\r\n",
    "    t_graph = g.sub_graph(t_cut, gen_uuid())\r\n",
    "    s_result = min_cut_scheduler.schedule(s_graph, sc.get_edge_domains()[0].topo)\r\n",
    "    t_result = min_cut_scheduler.schedule(t_graph, sc.get_cloud_domains()[0].topo)\r\n",
    "    if s_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"s_graph {} failed: {}\".format(g.uuid, s_result.reason))\r\n",
    "        continue\r\n",
    "    if t_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"t_graph {} failed: {}\".format(g.uuid, t_result.reason))\r\n",
    "        continue\r\n",
    "    result = sch.SchedulingResult.merge(s_result, t_result)\r\n",
    "    min_cut_calculator.add_scheduled_graph(g, result)\r\n",
    "min_cut_latency = min_cut_calculator.compute_latency()\r\n",
    "print(min_cut_latency)\r\n",
    "print(sum(min_cut_latency.values()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "graph 834f64c5: s 1 t 2\n",
      "graph da369a50: s 2 t 2\n",
      "graph bfc5ef50: s 1 t 2\n",
      "graph e863e49e: s 1 t 6\n",
      "graph 6888528a: s 2 t 5\n",
      "graph 7ba368e4: s 3 t 5\n",
      "graph bf7f03ac: s 8 t 1\n",
      "graph 55a4fc5d: s 1 t 9\n",
      "graph 86daf2e3: s 3 t 2\n",
      "graph 495d7f93: s 1 t 3\n",
      "{'834f64c5': 72.0, 'da369a50': 75.0, 'bfc5ef50': 75.0, 'e863e49e': 99.0, '6888528a': 78.0, '7ba368e4': 72.0, 'bf7f03ac': 75.0, '55a4fc5d': 78.0, '86daf2e3': 78.0, '495d7f93': 99.0}\n",
      "801.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "random_scheduler = sch.RandomScheduler()\r\n",
    "random_scheduler.logger.setLevel(logging.INFO)\r\n",
    "random_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "random_calculator.logger.setLevel(logging.INFO)\r\n",
    "random_result_list = []\r\n",
    "for g in graph_list:\r\n",
    "    result = random_scheduler.schedule(g, sc.topo)\r\n",
    "    if result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"graph {} failed: {}\".format(g.uuid, result.reason))\r\n",
    "    random_calculator.add_scheduled_graph(g, result)\r\n",
    "random_latency = random_calculator.compute_latency()\r\n",
    "print(random_latency)\r\n",
    "print(sum(random_latency.values()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'834f64c5': 93.0, 'da369a50': 105.0, 'bfc5ef50': 127.0, 'e863e49e': 328.0, '6888528a': 504.0, '7ba368e4': 334.0, 'bf7f03ac': 328.0, '55a4fc5d': 592.0, '86daf2e3': 328.0, '495d7f93': 185.0}\n",
      "2924.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "sc.topo.clear_occupied()\r\n",
    "all_edge_scheduler = sch.RandomScheduler()\r\n",
    "all_edge_scheduler.logger.setLevel(logging.INFO)\r\n",
    "all_edge_calculator = sch.LatencyCalculator(sc.topo)\r\n",
    "all_edge_calculator.logger.setLevel(logging.INFO)\r\n",
    "all_edge_result_list = []\r\n",
    "for g in graph_list:\r\n",
    "    t_cut = set([v.uuid for v in g.get_sinks()])\r\n",
    "    s_cut = set([v.uuid for v in g.get_vertexs()]) - t_cut\r\n",
    "    s_graph = g.sub_graph(s_cut, gen_uuid())\r\n",
    "    t_graph = g.sub_graph(t_cut, gen_uuid())\r\n",
    "    s_result = all_edge_scheduler.schedule(s_graph, sc.get_edge_domains()[0].topo)\r\n",
    "    t_result = all_edge_scheduler.schedule(t_graph, sc.get_cloud_domains()[0].topo)\r\n",
    "    if s_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"s_graph {} failed: {}\".format(g.uuid, s_result.reason))\r\n",
    "        continue\r\n",
    "    if t_result.status == sch.SchedulingResultStatus.FAILED:\r\n",
    "        print(\"t_graph {} failed: {}\".format(g.uuid, t_result.reason))\r\n",
    "        continue\r\n",
    "    result = sch.SchedulingResult.merge(s_result, t_result)\r\n",
    "    all_edge_calculator.add_scheduled_graph(g, result)\r\n",
    "all_edge_latency = all_edge_calculator.compute_latency()\r\n",
    "print(all_edge_latency)\r\n",
    "print(sum(all_edge_latency.values()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'834f64c5': 90.0, 'da369a50': 87.0, 'bfc5ef50': 165.0, 'e863e49e': 90.0, '6888528a': 90.0, '7ba368e4': 93.0, 'bf7f03ac': 105.0, '55a4fc5d': 96.0, '86daf2e3': 97.0, '495d7f93': 88.0}\n",
      "1001.0\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "interpreter": {
   "hash": "add1a61ff37513b0d798fa9b3ddbf8ae573dfe73fde377561c8d981e86c7d8d4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}